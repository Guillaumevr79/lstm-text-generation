{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddb7eb2",
   "metadata": {},
   "source": [
    "# TP : Implémentation d’un LSTM pour la génération de texte\n",
    "\n",
    "## Introduction\n",
    "Dans ce TP, vous allez mettre en œuvre un modèle de langage basé sur LSTM capable de générer du texte. \n",
    "Les étapes de ce projet sont : \n",
    "- Préparation des données, \n",
    "- Construction du vocabulaire,\n",
    "- Conception et Entraînement d’un modèle séquentiel (LSTM)\n",
    "- Test et Evaluation de ses performances.\n",
    "\n",
    "Corpus d'exemple en anglais et en français, de petite et grande taille, sont fourni pour expérimenter.\n",
    "\n",
    "### General Pipeline\n",
    "\n",
    "1. Préparer et tokeniser un corpus textuel pour un modèle séquentiel.\n",
    "2. Construire un vocabulaire mot par mot avec des tokens spéciaux (`<pad>`, `<unk>`, `<bos>`, `<eos>`).\n",
    "    - `unk_token`: the unknown token.\n",
    "    - `bos_token`: the beginning of sequence token.\n",
    "    - `eos_token`: the end of sequence token.\n",
    "    - `pad_token`: token to use for padding.\n",
    "4. Créer un `Dataset` PyTorch pour générer des séquences mot par mot.\n",
    "5. Implémenter et entraîner un modèle LSTM pour prédire le mot suivant.\n",
    "6. Générer du texte en utilisant différentes stratégies (`greedy`, `sampling`).\n",
    "7. Évaluer qualitativement et quantitativement le modèle (`perplexité`, `cohérence`, `diversité`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f17dfd",
   "metadata": {},
   "source": [
    "## 1. Environnement et bibliothèques\n",
    "Nous utiliserons `numpy`, `matplotlib`, `torch` (PyTorch) et `torch.utils.data`.\n",
    "Assurez-vous d’exécuter ce notebook dans un environnement avec PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7891c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db8fbb",
   "metadata": {},
   "source": [
    "## 2. Jeux de données d'exemple\n",
    "Nous avons quatre corpus pour expérimenter :\n",
    "- Small English (~200-400 mots)\n",
    "- Large English (~10k mots)\n",
    "- Small French (~200-400 mots)\n",
    "- Large French (~10k mots)\n",
    "\n",
    "Vous pouvez remplacer ces textes par vos propres corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machiavelli's The Prince (1513)\n",
    "small_english = '''\n",
    "You must know there are two ways of contesting, the one by the law, the other by force; the first method is proper to men, the second to beasts; but because the first is frequently not sufficient, it is necessary to have recourse to the second. Therefore it is necessary for a prince to understand how to avail himself of the beast and the man.\n",
    "'''.strip()\n",
    "\n",
    "large_english = ' '.join([small_english]*200)\n",
    "\n",
    "#Le Prince de Nicolas Machiavel: CHAPITRE XVIII.\n",
    "small_french = '''\n",
    "On peut combattre de deux manières: ou avec les lois, ou avec la force. La première est propre à l'homme, la seconde est celle des bêtes; mais comme souvent celle-là ne suffit point, on est obligé de recourir à l'autre: il faut donc qu'un prince sache agir à propos, et en bête et en homme.\n",
    "'''.strip()\n",
    "\n",
    "large_french = ' '.join([small_french]*200)\n",
    "\n",
    "datasets = {\n",
    "    'eng_small': small_english,\n",
    "    'eng_large': large_english,\n",
    "    'fr_small': small_french,\n",
    "    'fr_large': large_french\n",
    "}\n",
    "\n",
    "for name, txt in datasets.items():\n",
    "    words = txt.split()\n",
    "    print(f'{name}: {len(words)} words, sample= ' + ' '.join(words[:12]) + ('...' if len(words)>12 else ''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42043440",
   "metadata": {},
   "source": [
    "## 3. Prétraitement et création du vocabulaire\n",
    "- Tokenisation simple mot par mot.\n",
    "- Construction du vocabulaire avec tokens spéciaux.`<pad>`, `<unk>`, `<bos>`, `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de5def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    ponctuations = ['.', ',', ';', ':', '!', '?']\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        word = word.lower()\n",
    "        for p in ponctuations:\n",
    "            word = word.replace(p,'')\n",
    "        all_words.append(word)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        self.all_words = set()\n",
    "        \n",
    "        for text in texts:\n",
    "            words = tokenize(text)\n",
    "            self.all_words.update(words)\n",
    "        self.all_words = sorted(list(self.all_words))\n",
    "        \n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.idx2word[0] = '<pad>'\n",
    "        self.word2idx['<unk>'] = 1\n",
    "        self.idx2word[1] = '<unk>'\n",
    "        self.word2idx['<bos>'] = 2\n",
    "        self.idx2word[2] = '<bos>'\n",
    "        self.word2idx['<eos>'] = 3\n",
    "        self.idx2word[3] = '<eos>'\n",
    "        \n",
    "        for i, word in enumerate(self.all_words, start=4):\n",
    "            self.word2idx.update({word:i})\n",
    "            self.idx2word.update({i:word})\n",
    "\n",
    "        self.vocab_size = len(self.all_words) + 4\n",
    "\n",
    "    def encode(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        if idx not in self.idx2word:\n",
    "            return '<unk>'\n",
    "        return self.idx2word[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54eb325",
   "metadata": {},
   "source": [
    "## 4. Dataset PyTorch \n",
    "\n",
    "Construire des séquences d'entrée de longueur `seq_len` contenant des indices de mots, et la target est le mot suivant (`seq_len+1`). Les batches seront de forme `(batch_size, seq_len)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c17aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, vocab, seq_len):\n",
    "        self.vocab = vocab\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "        self.words = ['<bos>'] + tokenize(text) + ['<eos>']\n",
    "        self.encoded_words = [self.vocab.encode(word) for word in self.words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_words) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input = self.encoded_words[idx : idx + self.seq_len]\n",
    "        target = self.encoded_words[idx + self.seq_len]\n",
    "        return torch.tensor(input), torch.tensor(target)\n",
    "\"\"\"\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.pairs = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = ['<bos>'] + tokenize(sentence)  + ['<eos>']\n",
    "            encoded_words = []\n",
    "            for word in words:\n",
    "                idx = self.vocab.encode(word)\n",
    "                encoded_words.append(idx)\n",
    "\n",
    "            nb_pairs = len(encoded_words) - self.seq_len\n",
    "\n",
    "            for i in range(nb_pairs): \n",
    "                input_seq = encoded_words[i : i + self.seq_len]\n",
    "                target_seq = encoded_words[i + self.seq_len]\n",
    "                self.pairs.append((input_seq, target_seq))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.pairs[idx]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54ece4",
   "metadata": {},
   "source": [
    "## 5. Modèle LSTM pour la génération de texte\n",
    "\n",
    "Vous pouvez proposer votre propre architecture ou vous inspirer de l’architecture suivante : \n",
    "- `nn.Embedding(vocab_size, embed_dim)` : convertit indices → vecteurs.\n",
    "- `nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)` : traite la séquence.\n",
    "- `nn.Linear(hidden_dim, vocab_size)` : prédiction du mot suivant à chaque pas de temps (nous utilisons la sortie du dernier pas pour prédire le mot suivant du segment).\n",
    "- On ajoutera `Dropout` et une petite couche fully‑connected optionnelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #output, hidden = self.lstm(embedded, hidden)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.dropout(output)\n",
    "        linear = self.linear(output[:, -1, : ])\n",
    "\n",
    "        return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d020ff",
   "metadata": {},
   "source": [
    "## 6. Fonctions utilitaires\n",
    "\n",
    "- **Perplexity**: la perplexité est une métrique courante pour évaluer des modèles de langage : `perplexity = exp(loss)` où `loss` est la cross‑entropy moyenne par mot.\n",
    "- **Génération**: méthode de génération du texte mot par mot (greedy / sampling) : on propose deux modes\n",
    "    * `greedy` (choix du mot le plus probable)\n",
    "    * `sampling` (échantillonnage multinomial contrôlé par `temperature`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "def generate_words(model, vocab, prompt, max_length=20, mode='greedy', temp = 1.0, seq_len=5):\n",
    "    model.eval()\n",
    "    \n",
    "    prompt_tokens = tokenize(prompt)\n",
    "    tokens = [vocab.encode('<bos>')] + [vocab.encode(word) for word in prompt_tokens]\n",
    "    words = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        recent_tokens = tokens[-seq_len:]\n",
    "\n",
    "        if len(recent_tokens) < seq_len:\n",
    "            pad_idx = vocab.encode('<pad>')\n",
    "            padding = [pad_idx] * (seq_len - len(recent_tokens))\n",
    "            recent_tokens = padding + recent_tokens\n",
    "            \n",
    "        x = torch.tensor([recent_tokens])\n",
    "        logits = model(x)\n",
    "    \n",
    "        match mode :\n",
    "            case \"greedy\":\n",
    "                next_token = torch.argmax(logits).item()\n",
    "            case \"sampling\":\n",
    "                probs = torch.softmax(logits/temp, dim = 1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "    \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "        if next_token == vocab.encode('<eos>'):\n",
    "            break\n",
    "    \n",
    "    for token in tokens:\n",
    "        word = vocab.decode(token)\n",
    "        words.append(word)\n",
    "\n",
    "    return words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cd50b-7b05-4a5d-8a63-7905149aed07",
   "metadata": {},
   "source": [
    "## 7. Boucle d'entraînement générique\n",
    "\n",
    "La boucle d'entraînement :\n",
    "- Définit les paramètres et cycles d'entraînement.\n",
    "- Est applicable à tout jeu de données.\n",
    "- Calcule la perplexité moyenne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b00e8-48e7-4412-8cbc-1c850edb0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=10, batch_size=32, lr=0.001):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Perplexité: {perplexity(avg_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24013668-f716-43fc-b262-03a6cb1137a0",
   "metadata": {},
   "source": [
    "## 8. Expérimentation\n",
    "\n",
    "- Pour lancer une vraie expérience sur des grands dataset (exemple : `eng_large` ou `fr_large`), reconstruisez le vocabulaire avec ces textes, augmentez `embed_dim`, `hidden_dim`, `num_layers`, et le nombre d'époques.\n",
    "- Pour des textes plus longs par génération, augmentez `length` dans `generate_words`.\n",
    "- Pour réduire l'overfitting sur petits datasets : dropout plus élevé, régularisation, early stopping.\n",
    "\n",
    "## 9. Suggestions et améliorations \n",
    "\n",
    "- Utiliser des tokenizers plus robustes (BPETokenizer, spaCy) et traiter la casse/ponctuation.\n",
    "- Remplacer LSTM par Transformer pour de meilleurs résultats sur grands corpora.\n",
    "- Sauvegarder / charger des checkpoints (`torch.save` / `torch.load`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337ed60",
   "metadata": {},
   "source": [
    "## Importation et chargement des données\n",
    "\n",
    "- Importation des données (fra_sentences.tsv et Wikinews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#chargement de phrases à partir d'un fichier\n",
    "def load_sentences(file_path, num_sentences):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_sentences:\n",
    "                break\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                sentences.append(parts[2])\n",
    "    return sentences\n",
    "\n",
    "#Charger les phrases\n",
    "sentences = load_sentences('fra_sentences.tsv', num_sentences=50000)\n",
    "print(f\"Nombre de phrases chargées : {len(sentences)}\")\n",
    "print(f\"Exemples de phrases :\")\n",
    "for i in range(10):\n",
    "    print(f\" {i+1}. {sentences[i]}\")\n",
    "\n",
    "\"\"\"\n",
    "#Importer WikiNews\n",
    "import json\n",
    "\n",
    "def load_wikinews(file_path, num_samples=1000):\n",
    "    articles = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data[:num_samples]:\n",
    "            full_text = item['gold_ref']\n",
    "            articles.append(full_text)\n",
    "    return articles\n",
    "\n",
    "# 2. Charger pour l'entraînement\n",
    "wikinews_file = 'Dataset amp Evaluation Script-20260113/wikinews_typical-0.95_gpt2-xl_256.jsonl'\n",
    "wikinews_articles = load_wikinews(wikinews_file, num_samples=5000)\n",
    "\n",
    "print(f\"Nombre d'articles WikiNews chargés : {len(wikinews_articles)}\")\n",
    "print(f\"\\nExemple d'article :\")\n",
    "print(wikinews_articles[0][:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0293c85",
   "metadata": {},
   "source": [
    "## Test du modèle\n",
    "\n",
    "- Création du vocabulaire\n",
    "- Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Test sur dataset fra_sentences.tsv de tatoeba\n",
    "# 1. Vocabulaire\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(sentences)\n",
    "\n",
    "# 2. Dataset\n",
    "dataset = TextDataset(sentences, vocab, seq_len=5)\n",
    "\n",
    "# 3. Modèle\n",
    "model = LSTMModel(\n",
    "    vocab_size=vocab.vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# 4. Entraîner\n",
    "train(model, dataset, epochs=100, batch_size=64, lr=0.001)\n",
    "\n",
    "# 5. Générer\n",
    "#print(f\"Greedy : {generate_words(model, vocab, mode='greedy', max_length=30)}\")\n",
    "print(f\"Greedy : {generate_words(model, vocab, prompt='Je',mode='greedy', max_length=30)}\")\n",
    "\n",
    "#print(f\"Sampling : {generate_words(model, vocab, mode='sampling', temp=0.8, max_length=30)}\")\n",
    "print(f\"Sampling : {generate_words(model, vocab, prompt='Je',mode='sampling', temp=0.8, max_length=30)}\")\n",
    "\"\"\"\n",
    "\n",
    "# Vocabulaire\n",
    "vocab_wikinews = Vocabulary()\n",
    "vocab_wikinews.build_vocab(wikinews_articles)\n",
    "\n",
    "print(f\"Taille du vocabulaire : {vocab_wikinews.vocab_size}\")\n",
    "\n",
    "# Dataset\n",
    "dataset_wikinews = TextDataset(wikinews_articles, vocab_wikinews, seq_len=5)\n",
    "\n",
    "print(f\"Nombre de paires d'entraînement : {len(dataset_wikinews)}\")\n",
    "\n",
    "# Modèle\n",
    "model_wikinews = LSTMModel(\n",
    "    vocab_size=vocab_wikinews.vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# Entraîner\n",
    "train(model_wikinews, dataset_wikinews, epochs=50, batch_size=64, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e0bde",
   "metadata": {},
   "source": [
    "## Charger les prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikinews_prompts(file_path, num_samples=100):\n",
    "    prompts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data[:num_samples]:\n",
    "            prompts.append({\n",
    "                'prompt': item['prompt'],\n",
    "                'gold_ref': item['gold_ref']\n",
    "            })\n",
    "    return prompts\n",
    "\n",
    "# Charger les prompts de test\n",
    "test_prompts = load_wikinews_prompts(wikinews_file, num_samples=100)\n",
    "\n",
    "print(f\"Nombre de prompts de test : {len(test_prompts)}\")\n",
    "print(f\"\\nExemple de prompt :\")\n",
    "print(test_prompts[0]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2109c",
   "metadata": {},
   "source": [
    "## Générer des prédictions - sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prédictions avec SAMPLING\n",
    "results_sampling = []\n",
    "\n",
    "for idx, item in enumerate(test_prompts):\n",
    "    print(f\"Génération SAMPLING {idx+1}/{len(test_prompts)}\", end='\\r')\n",
    "    \n",
    "    prompt = item['prompt']\n",
    "    generated_results = {}\n",
    "    \n",
    "    for pred_idx in range(5):\n",
    "        gen = generate_words(\n",
    "            model_wikinews, \n",
    "            vocab_wikinews, \n",
    "            prompt=prompt, \n",
    "            max_length=50, \n",
    "            mode='sampling', \n",
    "            temp=0.8, \n",
    "            seq_len=5\n",
    "        )\n",
    "        generated_results[str(pred_idx)] = ' '.join(gen)\n",
    "    \n",
    "    results_sampling.append({\n",
    "        'prefix_text': prompt,\n",
    "        'generated_result': generated_results\n",
    "    })\n",
    "\n",
    "output_file_sampling = 'wikinews_lstm_predictions_sampling.json'\n",
    "with open(output_file_sampling, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_sampling, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Prédictions SAMPLING sauvegardées dans {output_file_sampling}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6777e7",
   "metadata": {},
   "source": [
    "## Evaluation - sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer SAMPLING\n",
    "print(\"=== ÉVALUATION SAMPLING ===\")\n",
    "!python \"Dataset amp Evaluation Script-20260113/_script_eval_/measure_diversity_mauve_gen_length.py\" --test_path wikinews_lstm_predictions_sampling.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18d4f6",
   "metadata": {},
   "source": [
    "## Générer des prédictions - greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prédictions avec GREEDY\n",
    "results_greedy = []\n",
    "\n",
    "for idx, item in enumerate(test_prompts):\n",
    "    print(f\"Génération GREEDY {idx+1}/{len(test_prompts)}\", end='\\r')\n",
    "    \n",
    "    prompt = item['prompt']\n",
    "    generated_results = {}\n",
    "    \n",
    "    for pred_idx in range(5):\n",
    "        gen = generate_words(\n",
    "            model_wikinews, \n",
    "            vocab_wikinews, \n",
    "            prompt=prompt, \n",
    "            max_length=50, \n",
    "            mode='greedy', \n",
    "            seq_len=5\n",
    "        )\n",
    "        generated_results[str(pred_idx)] = ' '.join(gen)\n",
    "    \n",
    "    results_greedy.append({\n",
    "        'prefix_text': prompt,\n",
    "        'generated_result': generated_results\n",
    "    })\n",
    "\n",
    "# Sauvegarder\n",
    "output_file_greedy = 'wikinews_lstm_predictions_greedy.json'\n",
    "with open(output_file_greedy, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_greedy, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Prédictions GREEDY sauvegardées dans {output_file_greedy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac8800e",
   "metadata": {},
   "source": [
    "## Evaluation - greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb48c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer GREEDY\n",
    "print(\"=== ÉVALUATION GREEDY ===\")\n",
    "!python \"Dataset amp Evaluation Script-20260113/_script_eval_/measure_diversity_mauve_gen_length.py\" --test_path wikinews_lstm_predictions_greedy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f82c9",
   "metadata": {},
   "source": [
    "## Sampling vs Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b613ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les résultats GREEDY vs SAMPLING\n",
    "import json\n",
    "\n",
    "# résultats greedy\n",
    "greedy_result_file = 'wikinews_lstm_predictions_greedy_diversity_mauve_gen_length_result.json'\n",
    "with open(greedy_result_file, 'r', encoding='utf-8') as f:\n",
    "    greedy_results = json.load(f)[0]\n",
    "\n",
    "# résultats sampling\n",
    "sampling_result_file = 'wikinews_lstm_predictions_sampling_diversity_mauve_gen_length_result.json'\n",
    "with open(sampling_result_file, 'r', encoding='utf-8') as f:\n",
    "    sampling_results = json.load(f)[0]\n",
    "\n",
    "# comparaison\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARAISON GREEDY vs SAMPLING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n DIVERSITÉ (Distinct-n)\")\n",
    "print(\"-\" * 60)\n",
    "div_greedy = greedy_results['diversity_dict']\n",
    "div_sampling = sampling_results['diversity_dict']\n",
    "print(f\"{'Métrique':<20} {'GREEDY':<15} {'SAMPLING':<15} {'Meilleur'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Distinct-1':<20} {div_greedy['distinct-1']:<15} {div_sampling['distinct-1']:<15} {'SAMPLING' if float(div_sampling['distinct-1']) > float(div_greedy['distinct-1']) else 'GREEDY'}\")\n",
    "print(f\"{'Distinct-2':<20} {div_greedy['distinct-2']:<15} {div_sampling['distinct-2']:<15} {'SAMPLING' if float(div_sampling['distinct-2']) > float(div_greedy['distinct-2']) else 'GREEDY'}\")\n",
    "print(f\"{'Distinct-3':<20} {div_greedy['distinct-3']:<15} {div_sampling['distinct-3']:<15} {'SAMPLING' if float(div_sampling['distinct-3']) > float(div_greedy['distinct-3']) else 'GREEDY'}\")\n",
    "\n",
    "print(\"\\n LONGUEUR DE GÉNÉRATION\")\n",
    "print(\"-\" * 60)\n",
    "len_greedy = greedy_results['gen_length_dict']\n",
    "len_sampling = sampling_results['gen_length_dict']\n",
    "print(f\"{'Métrique':<20} {'GREEDY':<15} {'SAMPLING':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Longueur moyenne':<20} {len_greedy['average_gen_length']:<15} {len_sampling['average_gen_length']:<15}\")\n",
    "\n",
    "print(\"\\n MAUVE SCORE\")\n",
    "print(\"-\" * 60)\n",
    "mauve_greedy = greedy_results['mauve_dict']\n",
    "mauve_sampling = sampling_results['mauve_dict']\n",
    "print(f\"{'Métrique':<20} {'GREEDY':<15} {'SAMPLING':<15} {'Meilleur'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'MAUVE':<20} {mauve_greedy['mauve']:<15} {mauve_sampling['mauve']:<15} {'GREEDY' if float(mauve_greedy['mauve']) > float(mauve_sampling['mauve']) else 'SAMPLING'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" INTERPRÉTATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"• Distinct-n : Plus élevé = plus de diversité (mieux)\")\n",
    "print(\"• MAUVE : Plus proche de 1 = plus proche du texte de référence (mieux)\")\n",
    "print(\"• Longueur : Dépend de la tâche\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68976e36-3d9d-47f4-bf7a-bba8b53d4477",
   "metadata": {},
   "source": [
    "# Annexe:\n",
    "\n",
    "## 1. Small English Datasets\n",
    "\n",
    "| Dataset                 | Size    | Description / Domain                                                                  | Link                                                                                                                                                                                  |\n",
    "| ----------------------- | ------- | ------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Tiny Shakespeare**    | ~1 MB   | Shakespeare’s plays (~100k characters). Great for character-level LSTM experiments.   | [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) |\n",
    "| **Alice in Wonderland** | ~160 KB | Classic novel by Lewis Carroll. Good for story-style text generation.                 | [Project Gutenberg](https://www.gutenberg.org/ebooks/11)                                                                                                                               |\n",
    "| **BBC News Summary**    | ~5 MB   | Short news articles, varied topics. Useful for sentence-level generation experiments. | [Kaggle](https://www.kaggle.com/datasets/pariza/bbc-news-summary)                                                                                                                      |\n",
    "\n",
    "## 2. Small French Datasets\n",
    "\n",
    "| Dataset                                 | Size                | Description / Domain                                                                  | Link                                                                                                         |\n",
    "| --------------------------------------- | ------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **Le Petit Prince**                     | ~50 KB              | Classic French children’s novel. Ideal for simple French text generation experiments. | [Project Gutenberg FR](https://www.gutenberg.org/ebooks/12910)                                               |\n",
    "| **88milSMS (sample)**                   | ~ 1,000 SMS (~50 KB) | Informal French text messages. Good for casual/conversational style experiments.      | [Sample download](https://github.com/laurentprudhon/frenchtext)                                              |\n",
    "| **French News Articles (small sample)** | ~200 KB             | Short news articles from French news outlets.                                         | Can be sampled from [frenchtext](https://laurentprudhon.github.io/frenchtext/datasets/index.html) |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
