{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4e96d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "afc028a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, dropout=0.3, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = None\n",
    "        self.lstm = None\n",
    "        self.dropout = None\n",
    "        self.output_layer = None\n",
    "        self.vocab_size = None\n",
    "        self.criterion = None\n",
    "        \n",
    "        # Dictionnaires\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "\n",
    "    def _initialize_layers(self):\n",
    "        if self.vocab_size is None:\n",
    "            raise ValueError(\"Vous devez d'abord appeler build_vocabulary()\")\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # bidirectional=False pour lire dans 1 sens\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.embedding_dim, \n",
    "            self.hidden_size, \n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True, \n",
    "            dropout=self.dropout_rate if self.num_layers > 1 else 0,\n",
    "            bidirectional=False \n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        print(f\"Modèle prêt : vocab={self.vocab_size}, hidden={self.hidden_size}, Unidirectionnel\")\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # On permet de passer un état caché (hidden) pour la génération efficace\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Si hidden est fourni, on l'utilise, sinon LSTM l'initialise à 0\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.output_layer(lstm_out)\n",
    "        \n",
    "        # On retourne aussi l'état caché pour la prochaine étape de génération\n",
    "        return output, hidden\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
    "        \n",
    "        word_counts = {}\n",
    "        for text in texts:\n",
    "            # Simple tokenization par espace\n",
    "            for word in text.split():\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        all_words = sorted([w for w, c in word_counts.items() if c >= 2])\n",
    "        final_vocab = special_tokens + all_words\n",
    "        \n",
    "        self.word_to_idx = {word: i for i, word in enumerate(final_vocab)}\n",
    "        self.idx_to_word = {i: word for i, word in enumerate(final_vocab)}\n",
    "        \n",
    "        self.vocab_size = len(final_vocab)\n",
    "        print(f\"Vocabulaire créé : {self.vocab_size} mots\")\n",
    "        \n",
    "        self._initialize_layers()\n",
    "    \n",
    "    def text_to_indices(self, text):\n",
    "        words = text.split()\n",
    "        sequence = [\"<BOS>\"] + words + [\"<EOS>\"]\n",
    "        return [self.word_to_idx.get(word, self.word_to_idx[\"<UNK>\"]) for word in sequence]\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            word = self.idx_to_word.get(idx, \"\")\n",
    "            if word not in [\"<BOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\", \"\"]:\n",
    "                words.append(word)\n",
    "        result = \" \".join(words)\n",
    "        result = re.sub(r\"\\s+'\", \"'\", result)\n",
    "        result = re.sub(r\"'\\s+\", \"'\", result)\n",
    "        return result\n",
    "\n",
    "    def pad_sequence(self, indices, max_length):\n",
    "        if len(indices) >= max_length:\n",
    "            return indices[:max_length]\n",
    "        return indices + [0] * (max_length - len(indices))\n",
    "\n",
    "    def create_training_pair(self, sequence):\n",
    "        # Gestion du cas où sequence ne contient pas EOS (juste par sécurité)\n",
    "        if self.word_to_idx[\"<EOS>\"] in sequence:\n",
    "            eos_idx = sequence.index(self.word_to_idx[\"<EOS>\"])\n",
    "            input_seq = sequence[:eos_idx]\n",
    "            target_seq = sequence[1:eos_idx + 1]\n",
    "        else:\n",
    "            # Fallback si pas de EOS\n",
    "            input_seq = sequence[:-1]\n",
    "            target_seq = sequence[1:]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "    def prepare_batch(self, texts):\n",
    "        all_inputs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for text in texts:\n",
    "            indices = self.text_to_indices(text)\n",
    "            if len(indices) < 2: continue # Skip trop court\n",
    "            input_seq, target_seq = self.create_training_pair(indices)\n",
    "            all_inputs.append(input_seq)\n",
    "            all_targets.append(target_seq)\n",
    "        \n",
    "        if not all_inputs: return None, None\n",
    "\n",
    "        max_len = max(len(seq) for seq in all_inputs)\n",
    "        padded_inputs = [self.pad_sequence(seq, max_len) for seq in all_inputs]\n",
    "        padded_targets = [self.pad_sequence(seq, max_len) for seq in all_targets]\n",
    "        \n",
    "        return torch.tensor(padded_inputs), torch.tensor(padded_targets)\n",
    "\n",
    "    def train_step(self, input_batch, target_batch):\n",
    "        # Le forward retourne un tuple (output, hidden)\n",
    "        predictions, _ = self(input_batch) \n",
    "        \n",
    "        predictions_flat = predictions.view(-1, self.vocab_size)\n",
    "        targets_flat = target_batch.view(-1)\n",
    "        \n",
    "        loss = self.criterion(predictions_flat, targets_flat)\n",
    "        return loss\n",
    "    \n",
    "    def generate_text(self, start_text, max_length=20, temperature=1.0, top_k=10, repetition_penalty=1.2):\n",
    "        self.eval()\n",
    "        \n",
    "        # Récupération du device (CPU, CUDA ou MPS) pour éviter les erreurs de tenseur\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        current_seq = self.text_to_indices(start_text)\n",
    "        \n",
    "        # Retrait du token EOS s'il est présent à la fin\n",
    "        if current_seq and current_seq[-1] == self.word_to_idx.get(\"<EOS>\"):\n",
    "            current_seq = current_seq[:-1]\n",
    "            \n",
    "        generated_words = []\n",
    "        hidden = None\n",
    "        \n",
    "        # Conversion initiale sur le bon device\n",
    "        input_tensor = torch.tensor([current_seq], device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden = self(input_tensor, hidden)\n",
    "            next_token_logits = output[0, -1, :]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            logits = next_token_logits.clone()\n",
    "            \n",
    "            # 1. Bloquer tokens spéciaux\n",
    "            for token in [\"<PAD>\", \"<UNK>\", \"<BOS>\"]:\n",
    "                if token in self.word_to_idx:\n",
    "                    logits[self.word_to_idx[token]] = float('-inf')\n",
    "\n",
    "            # 2. Pénalité de répétition (fenêtre de 5 mots)\n",
    "            for word_idx in generated_words[-5:]:\n",
    "                if logits[word_idx] > 0:\n",
    "                    logits[word_idx] /= repetition_penalty\n",
    "                else:\n",
    "                    logits[word_idx] *= repetition_penalty\n",
    "            \n",
    "            # 3. RÈGLES GRAMMATICALES FORCÉES (Logic Patching)\n",
    "            if generated_words:\n",
    "                last_idx = generated_words[-1]\n",
    "                last_word = self.idx_to_word.get(last_idx, \"\")\n",
    "                \n",
    "                # Règle : Après \"que je\", on évite l'indicatif (suis, vais, sais...)\n",
    "                if last_word == \"je\" and len(generated_words) >= 2:\n",
    "                    prev_idx = generated_words[-2]\n",
    "                    prev_word = self.idx_to_word.get(prev_idx, \"\")\n",
    "                    \n",
    "                    if prev_word == \"que\":\n",
    "                        # On réduit drastiquement la probabilité de l'indicatif\n",
    "                        indicatifs = [\"suis\", \"sais\", \"vais\", \"peux\", \"veux\", \"ai\"]\n",
    "                        for verb in indicatifs:\n",
    "                            if verb in self.word_to_idx:\n",
    "                                logits[self.word_to_idx[verb]] -= 5.0 # Pénalité forte (soustraction sur les logits)\n",
    "            \n",
    "            # Sampling\n",
    "            logits = logits / temperature\n",
    "            top_k_actual = min(top_k, self.vocab_size)\n",
    "            filtered_logits, top_k_indices = torch.topk(logits, top_k_actual)\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)\n",
    "            \n",
    "            predicted_idx_in_topk = torch.multinomial(probs, 1).item()\n",
    "            predicted_word_idx = top_k_indices[predicted_idx_in_topk].item()\n",
    "            \n",
    "            if predicted_word_idx == self.word_to_idx.get(\"<EOS>\"):\n",
    "                break\n",
    "            \n",
    "            generated_words.append(predicted_word_idx)\n",
    "            \n",
    "            # Préparation itération suivante\n",
    "            input_tensor = torch.tensor([[predicted_word_idx]], device=device)\n",
    "            with torch.no_grad():\n",
    "                output, hidden = self(input_tensor, hidden)\n",
    "                next_token_logits = output[0, -1, :]\n",
    "        \n",
    "        self.train()\n",
    "        return self.indices_to_text(current_seq + generated_words)\n",
    "\n",
    "    def generate_best(self, prompt, n_candidates=5, max_length=15, temperature=0.7, top_k=15):\n",
    "        \"\"\"Génère plusieurs candidats et retourne le plus cohérent selon des critères heuristiques\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for _ in range(n_candidates):\n",
    "            text = self.generate_text(prompt, max_length, temperature, top_k)\n",
    "            candidates.append(text)\n",
    "        \n",
    "        def score_coherence(text):\n",
    "            score = 0\n",
    "            words = text.split()\n",
    "            \n",
    "            # Critère 1: Longueur (pour éviter les trucs trop courts)\n",
    "            if len(words) < 4: score -= 10\n",
    "            \n",
    "            # Critère 2: Répétitions strictes\n",
    "            if len(words) != len(set(words)): score -= 5\n",
    "            \n",
    "            # Critère 3: Fin de phrase abrupte (prépositions/articles à la fin)\n",
    "            if words[-1] in [\"le\", \"la\", \"les\", \"un\", \"une\", \"de\", \"à\", \"que\", \"qui\", \"pour\", \"et\"]:\n",
    "                score -= 8\n",
    "            \n",
    "            # Critère 4: Erreurs grammaticales connues (Correction des logs précédents)\n",
    "            bad_phrases = [\n",
    "                \"faut que je suis\", \"faut que je sais\", \"faut que je vais\", # Subjonctif incorrect\n",
    "                \"faut que je me suis\", \n",
    "                \"acheter une heure\", # Non-sens sémantique\n",
    "                \"parler à moi\", # Syntaxique (me parler)\n",
    "                \"sais pas le\", \n",
    "                \"démontrer vu\"\n",
    "            ]\n",
    "            \n",
    "            for bad in bad_phrases:\n",
    "                if bad in text:\n",
    "                    score -= 20  # Grosse pénalité\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # On trie les candidats du meilleur score au pire\n",
    "        candidates.sort(key=score_coherence, reverse=True)\n",
    "        \n",
    "        # (Optionnel) Afficher les candidats pour debug\n",
    "        # print(f\"--- Debug '{prompt}' ---\")\n",
    "        # for c in candidates:\n",
    "        #     print(f\"Score {score_coherence(c)}: {c}\")\n",
    "            \n",
    "        return candidates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fa7d6",
   "metadata": {},
   "source": [
    "Chargement et Nettoyage du Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "sum5xh9x08h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHARGEMENT ET NETTOYAGE DU CORPUS\n",
      "============================================================\n",
      "✓ Chargé 100000 phrases françaises\n",
      "\n",
      "Exemples:\n",
      "  1. je ne supporte pas ce type\n",
      "  2. ne tenez aucun compte de ce qu' il dit\n",
      "  3. qu' est-ce que tu fais\n",
      "  4. qu' est-ce que c' est\n",
      "  5. je serai bientôt de retour\n",
      "  6. je ne sais pas\n",
      "  7. j' en perds mes mots\n",
      "  8. ça ne va jamais finir\n",
      "  9. c’était un méchant lapin\n",
      "  10. j' étais dans les montagnes\n",
      "  11. est-ce que c' est une photo récente\n",
      "  12. je ne sais pas si j' ai le temps\n",
      "  13. pour une certaine raison le microphone ne marchait pas tout à l' heure\n",
      "  14. tout le monde doit apprendre par soi-même en fin de compte\n",
      "  15. l' éducation dans ce monde me déçoit\n",
      "  16. l' apprentissage ne devrait pas être forcé l' apprentissage devrait être encouragé\n",
      "  17. ça ne changera rien\n",
      "  18. il se peut que j' abandonne bientôt et fasse une sieste à la place\n",
      "  19. c' est parce que tu ne veux pas être seul\n",
      "  20. ça n' arrivera pas\n",
      "  21. parfois il peut être un gars bizarre\n",
      "  22. je ferai de mon mieux pour ne pas perturber tes révisions\n",
      "  23. je suppose que c' est différent quand tu y penses à long terme\n",
      "  24. ne t' inquiète pas\n",
      "  25. je les appellerai demain quand je reviendrai\n",
      "  26. j' ai toujours plus apprécié les personnages mystérieux\n",
      "  27. je leur ai dit de m' envoyer un autre ticket\n",
      "  28. tu es tellement impatiente avec moi\n",
      "  29. je ne peux pas vivre comme ça\n",
      "  30. il fut un temps où je voulais être astrophysicien\n",
      "  31. je n' ai jamais aimé la biologie\n",
      "  32. c' est malheureusement vrai\n",
      "  33. ils sont trop occupés à se battre entre eux pour s' occuper d' idéaux communs\n",
      "  34. la plupart des gens pensent que je suis fou\n",
      "  35. non je ne le suis pas c' est toi qui l' es\n",
      "  36. c' est ma réplique\n",
      "  37. il me donne des coups de pied\n",
      "  38. est-ce que tu es sûr\n",
      "  39. oh il y a un papillon\n",
      "  40. ça ne me surprend pas\n",
      "  41. pour une raison que j' ignore je me sens plus vivant la nuit\n",
      "  42. ça dépend du contexte\n",
      "  43. est-ce que tu te fous de moi\n",
      "  44. c' est la chose la plus stupide que j' ai jamais dite\n",
      "  45. je ne veux pas être lamentable je veux être cool\n",
      "  46. quand je serai grand je veux être roi\n",
      "  47. je suis si gros\n",
      "  48. je vais le descendre\n",
      "  49. je ne suis pas un vrai poisson je ne suis qu' une simple peluche\n",
      "  50. je ne fais que parler\n",
      "  51. c' était probablement ce qui a influencé leur décision\n",
      "  52. je me suis toujours demandé ce que ça ferait d' avoir des frères et sœurs\n",
      "  53. c' est ce que j' aurais dit\n",
      "  54. ça me prendrait l' éternité pour tout expliquer\n",
      "  55. c' est parce que tu es une fille\n",
      "  56. parfois je ne peux pas m' empêcher de montrer des émotions\n",
      "  57. c' est un mot pour lequel j' aimerais trouver un substitut\n",
      "  58. ce serait quelque chose qu' il faudrait que je programme\n",
      "  59. il n' est pas dans mon intention d' être égoïste\n",
      "  60. réfléchissons au pire qui pourrait arriver\n",
      "  61. combien d' amis proches est-ce que tu as\n",
      "  62. c' est toujours tel que ça a été\n",
      "  63. je pense que c' est mieux de ne pas être impoli\n",
      "  64. on peut toujours trouver du temps\n",
      "  65. je serais malheureux mais je ne me suiciderais pas\n",
      "  66. quand je me suis réveillé j' étais triste\n",
      "  67. c’est ce qui est expliqué à la fin\n",
      "  68. je pensais que tu aimais apprendre de nouvelles choses\n",
      "  69. la plupart des gens écrivent à propos de leur vie quotidienne\n",
      "  70. si je pouvais t' envoyer un marshmallow trang je le ferais\n",
      "  71. pour faire cela il te faut prendre des risques\n",
      "  72. chaque personne qui est seule est seule parce qu' elle a peur des autres\n",
      "  73. je ne suis pas un artiste je n' ai jamais eu l' esprit pour ça\n",
      "  74. je ne peux pas lui dire maintenant ce n’est pas si simple\n",
      "  75. je suis une personne qui a des défauts mais ces défauts peuvent être facilement corrigés\n",
      "  76. chaque fois que je trouve quelque chose que j' aime c' est trop cher\n",
      "  77. combien de temps es-tu restée\n",
      "  78. peut-être que ce sera la même chose pour lui\n",
      "  79. l' innocence est une belle chose\n",
      "  80. les humains n' ont jamais été faits pour vivre éternellement\n",
      "  81. je pense que j' ai une théorie à ce sujet\n",
      "  82. tu es en train de dire que tu caches intentionnellement ta beauté\n",
      "  83. je n’ai pas de compte sur ces forums\n",
      "  84. je ne savais pas d' où ça venait\n",
      "  85. ce n' est pas important\n",
      "  86. je n' ai pas aimé\n",
      "  87. elle demande comment c’est possible\n",
      "  88. tu ne fais que fuir les problèmes de la vie\n",
      "  89. si tu regardes les paroles elles ne signifient pas vraiment grand-chose\n",
      "  90. il y a un problème que tu ne vois pas\n",
      "  91. tu peux le faire\n",
      "  92. mon prof de physique s' en fiche si je sèche les cours\n",
      "  93. j' aimerais bien pouvoir aller au japon\n",
      "  94. je déteste ça quand il y a trop de gens\n",
      "  95. je dois aller au lit\n",
      "  96. je ne t' en demanderai pas plus pour aujourd' hui\n",
      "  97. il se peut qu' il gèle la semaine prochaine\n",
      "  98. même s' il s' est excusé je suis encore en colère\n",
      "  99. la police vous fera trouver les balles\n",
      "  100. merci de m' avoir expliqué finalement pourquoi on me prend pour une imbécile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHARGEMENT ET NETTOYAGE DU CORPUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "csv_path = \"/Users/guillaume/Docs/Formations/Intro-Deep-Learning/sentences.csv\"\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 1. Exclure les phrases problématiques\n",
    "    excluded = ['muiriel', 'www', 'http', '@', '#', '...', '..', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    if any(ex in sentence for ex in excluded):\n",
    "        return None\n",
    "    \n",
    "    # 2. Gestion intelligente de la ponctuation pour le français\n",
    "    # On garde les apostrophes mais on les sépare, et on vire le reste\n",
    "    # Cette regex remplace \"l'école\" par \"l' école\" et \"j'aime\" par \"j' aime\"\n",
    "    sentence = re.sub(r\"(['])\", r\"\\1 \", sentence) \n",
    "    \n",
    "    # Suppression de la ponctuation (sauf apostrophes qu'on a traitées)\n",
    "    # On garde aussi les tirets pour les mots composés (ex: arc-en-ciel)\n",
    "    sentence = re.sub(r'[«»\"\"„\\.\\,\\!\\?\\;\\:\\(\\)\\[\\]\\{\\}]', ' ', sentence)\n",
    "    \n",
    "    # Réduction des espaces multiples\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    \n",
    "    words = sentence.split()\n",
    "    \n",
    "    # 3. Filtre de longueur\n",
    "    # 5000 phrases c'est peu, donc on reste sur des phrases courtes à moyennes\n",
    "    if not (4 <= len(words) <= 15): \n",
    "        return None\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    french_sentences = []\n",
    "    seen = set()\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                lang_code = parts[1]\n",
    "                sentence = parts[2]\n",
    "                \n",
    "                if lang_code == 'fra':\n",
    "                    cleaned = clean_sentence(sentence)\n",
    "                    \n",
    "                    if cleaned and cleaned not in seen:\n",
    "                        seen.add(cleaned)\n",
    "                        french_sentences.append(cleaned)\n",
    "                \n",
    "                if len(french_sentences) >= 100000:\n",
    "                    break\n",
    "    \n",
    "    texts = french_sentences\n",
    "    \n",
    "    print(f\"✓ Chargé {len(texts)} phrases françaises\")\n",
    "    print(f\"\\nExemples:\")\n",
    "    for i in range(100):\n",
    "        print(f\"  {i+1}. {texts[i]}\")\n",
    "else:\n",
    "    print(f\"⚠ Fichier non trouvé!\")\n",
    "    texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16a508",
   "metadata": {},
   "source": [
    "Configuration sur 5k phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06750fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "# Détection automatique GPU/MPS (Mac M1/M2)/CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Utilisation forcé du device : {device} (pour stabilité Embedding)\")\n",
    "\n",
    "if len(texts) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIGURATION DU MODÈLE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # embedding 256 / hidden 256 est suffisant pour < 100k phrases\n",
    "    model = LSTMLanguageModel(\n",
    "        embedding_dim=256,\n",
    "        hidden_size=256, \n",
    "        dropout=0.4,       # Augmenté un peu pour forcer la généralisation\n",
    "        num_layers=2\n",
    "    ).to(device)\n",
    "    \n",
    "    model.build_vocabulary(texts)\n",
    "    \n",
    "    # Optimiseur et Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Préparation globale des données\n",
    "    # Note : Idéalement faire le padding par batch, mais ici simplifié\n",
    "    full_inputs, full_targets = model.prepare_batch(texts)\n",
    "    \n",
    "    # Création d'un Dataset et DataLoader pour gérer les mini-batchs facilement\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    \n",
    "    dataset = TensorDataset(full_inputs, full_targets)\n",
    "    # batch_size=64 est un bon standard (32 si manque de RAM, 128 si GPU puissant)\n",
    "    batch_size = 64 \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f\"\\nDonnées totales: {len(full_inputs)} phrases\")\n",
    "    print(f\"Taille des batchs: {batch_size}\")\n",
    "    print(f\"Nombre de batchs par époque: {len(dataloader)}\")\n",
    "    print(f\"Paramètres du modèle: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    print(\"\\n=== ENTRAÎNEMENT AVEC MINI-BATCHS ===\")\n",
    "    \n",
    "    model.train()\n",
    "    num_epochs = 100\n",
    "    \n",
    "    history_loss = []\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch_inputs, batch_targets in dataloader:\n",
    "                # Envoi des batchs sur le device (GPU/CPU)\n",
    "                batch_inputs = batch_inputs.to(device)\n",
    "                batch_targets = batch_targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward & Loss\n",
    "                loss = model.train_step(batch_inputs, batch_targets)\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clipping pour éviter l'explosion des gradients (important pour LSTM)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            # Calcul de la moyenne de la loss pour cette époque\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            history_loss.append(avg_loss)\n",
    "            \n",
    "            # Le scheduler ajuste le learning rate en fonction de la loss moyenne\n",
    "            scheduler.step(avg_loss)\n",
    "            \n",
    "            # Affichage périodique\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f} | LR: {lr:.6f} | Temps: {elapsed:.2f}s\")\n",
    "                \n",
    "                # Petit test de génération en direct pour voir les progrès !\n",
    "                if (epoch + 1) % 20 == 0:\n",
    "                    print(f\"   >>> Génération test : {model.generate_text('je suis', max_length=10)}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEntraînement interrompu manuellement.\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ENTRAÎNEMENT TERMINÉ - Loss finale: {history_loss[-1]:.4f}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19d87f",
   "metadata": {},
   "source": [
    "Configuration sur 100k phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a8daae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration : Device=cpu | Target=100000 phrases\n",
      " Lecture de fra_sentences.tsv...\n",
      " Chargé 100000 phrases propres.\n",
      " Train: 90000 | Validation: 10000\n",
      "Vocabulaire créé : 16434 mots\n",
      "Modèle prêt : vocab=16434, hidden=512, Unidirectionnel\n",
      "\n",
      " Démarrage de l'entraînement (20 époques)...\n",
      "Epoch 01 | Train Loss: 5.3411 | Val Loss: 4.5660 | 332s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 02 | Train Loss: 4.6411 | Val Loss: 4.2936 | 1211s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 03 | Train Loss: 4.4164 | Val Loss: 4.1610 | 272s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 04 | Train Loss: 4.2758 | Val Loss: 4.0784 | 194s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 05 | Train Loss: 4.1744 | Val Loss: 4.0245 | 183s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "   >>> Test: je suis désolé de ne pas te voir\n",
      "Epoch 06 | Train Loss: 4.0925 | Val Loss: 3.9892 | 167s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 07 | Train Loss: 4.0272 | Val Loss: 3.9629 | 429s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 08 | Train Loss: 3.9690 | Val Loss: 3.9383 | 181s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 09 | Train Loss: 3.9212 | Val Loss: 3.9255 | 187s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 10 | Train Loss: 3.8759 | Val Loss: 3.9135 | 172s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "   >>> Test: je suis un étranger\n",
      "Epoch 11 | Train Loss: 3.8375 | Val Loss: 3.9026 | 167s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 12 | Train Loss: 3.8008 | Val Loss: 3.8992 | 166s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 13 | Train Loss: 3.7683 | Val Loss: 3.8945 | 166s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 14 | Train Loss: 3.7389 | Val Loss: 3.8871 | 165s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 15 | Train Loss: 3.7098 | Val Loss: 3.8931 | 188s\n",
      "   >>> Test: je suis allé en france\n",
      "Epoch 16 | Train Loss: 3.6851 | Val Loss: 3.8883 | 166s\n",
      "Epoch 17 | Train Loss: 3.6607 | Val Loss: 3.8870 | 165s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 18 | Train Loss: 3.5898 | Val Loss: 3.8810 | 164s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 19 | Train Loss: 3.5639 | Val Loss: 3.8800 | 164s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "Epoch 20 | Train Loss: 3.5449 | Val Loss: 3.8796 | 164s\n",
      "   ★ Nouveau meilleur modèle sauvegardé !\n",
      "   >>> Test: je suis allé skier\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "FILE_PATH = \"fra_sentences.tsv\"  # <--- CHEMIN COMPLET ICI SI BESOIN\n",
    "LIMIT_SENTENCES = 100000         # Objectif : 100k phrases\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 256              \n",
    "HIDDEN_SIZE = 512                \n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5                    # Fort dropout pour éviter le par-cœur\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "# Force CPU pour éviter le bug MPS (Embedding)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\" Configuration : Device={device} | Target={LIMIT_SENTENCES} phrases\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. FONCTIONS DE NETTOYAGE & CHARGEMENT\n",
    "# ==========================================\n",
    "def clean_sentence_robust(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    # Exclure caractères bizarres et chiffres isolés\n",
    "    if any(c in sentence for c in ['@', '#', 'www', 'http', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
    "        return None\n",
    "    # Séparer les apostrophes (l'école -> l' école)\n",
    "    sentence = re.sub(r\"(['])\", r\"\\1 \", sentence)\n",
    "    # Nettoyer ponctuation\n",
    "    sentence = re.sub(r'[«»\"\"„\\.\\,\\!\\?\\;\\:\\(\\)\\[\\]\\{\\}]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    # Filtre longueur (ni trop court, ni trop long)\n",
    "    words = sentence.split()\n",
    "    if not (4 <= len(words) <= 18): \n",
    "        return None\n",
    "    return ' '.join(words)\n",
    "\n",
    "def load_tatoeba(filepath, limit):\n",
    "    print(f\" Lecture de {filepath}...\")\n",
    "    sentences = []\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\" ERREUR : Fichier introuvable à : {filepath}\")\n",
    "        return []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            # Tatoeba format : ID \\t LANG \\t TEXT\n",
    "            # Vérifier que c'est bien du français ('fra')\n",
    "            if len(parts) == 3 and parts[1] == 'fra':\n",
    "                text = parts[2]\n",
    "                cleaned = clean_sentence_robust(text)\n",
    "                if cleaned:\n",
    "                    sentences.append(cleaned)\n",
    "            \n",
    "            if len(sentences) >= limit:\n",
    "                break\n",
    "    \n",
    "    print(f\" Chargé {len(sentences)} phrases propres.\")\n",
    "    return sentences\n",
    "\n",
    "# ==========================================\n",
    "# 3. PRÉPARATION DES DONNÉES\n",
    "# ==========================================\n",
    "texts = load_tatoeba(FILE_PATH, LIMIT_SENTENCES)\n",
    "\n",
    "if len(texts) > 0:\n",
    "    # Train / Validation Split (90% / 10%)\n",
    "    random.seed(42)\n",
    "    random.shuffle(texts)\n",
    "    split_idx = int(len(texts) * 0.9)\n",
    "    train_texts = texts[:split_idx]\n",
    "    val_texts = texts[split_idx:]\n",
    "    \n",
    "    print(f\" Train: {len(train_texts)} | Validation: {len(val_texts)}\")\n",
    "\n",
    "    # Initialisation du modèle \n",
    "    model = LSTMLanguageModel(EMBEDDING_DIM, HIDDEN_SIZE, DROPOUT, NUM_LAYERS).to(device)\n",
    "    model.build_vocabulary(train_texts)\n",
    "\n",
    "    # Création des DataLoaders\n",
    "    train_inputs, train_targets = model.prepare_batch(train_texts)\n",
    "    val_inputs, val_targets = model.prepare_batch(val_texts)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_targets)\n",
    "    val_data = TensorDataset(val_inputs, val_targets)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE) # Pas besoin de shuffle pour la validation\n",
    "\n",
    "    # Optimiseur\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. BOUCLE D'ENTRAÎNEMENT (TRAIN + VAL)\n",
    "    # ==========================================\n",
    "    print(f\"\\n Démarrage de l'entraînement ({EPOCHS} époques)...\")\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    history_train = []\n",
    "    history_val = []\n",
    "\n",
    "    try:\n",
    "        for epoch in range(EPOCHS):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # --- PHASE TRAIN ---\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss = model.train_step(batch_x, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            \n",
    "            # --- PHASE VALIDATION ---\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    # Calcul manuel de la loss pour la validation\n",
    "                    predictions, _ = model(batch_x)\n",
    "                    # Redimensionner (view) pour que la Loss function accepte les données\n",
    "                    loss = model.criterion(predictions.view(-1, model.vocab_size), batch_y.view(-1))\n",
    "                    total_val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            \n",
    "            # --- LOGS & SAVE ---\n",
    "            history_train.append(avg_train_loss)\n",
    "            history_val.append(avg_val_loss)\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | {duration:.0f}s\")\n",
    "            \n",
    "            # Sauvegarde si record battu\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'vocab_word_to_idx': model.word_to_idx,\n",
    "                    'vocab_idx_to_word': model.idx_to_word,\n",
    "                    'vocab_size': model.vocab_size,\n",
    "                    'embedding_dim': EMBEDDING_DIM,\n",
    "                    'hidden_size': HIDDEN_SIZE,\n",
    "                    'num_layers': NUM_LAYERS,\n",
    "                    'dropout': DROPOUT\n",
    "                }, \"best_model_100k.pth\")\n",
    "                print(\"   ★ Nouveau meilleur modèle sauvegardé !\")\n",
    "            \n",
    "            # Test intermédiaire (toutes les 5 époques)\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                print(f\"   >>> Test: {model.generate_best('je suis', max_length=10)}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Entraînement stoppé manuellement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d00ffbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST AVEC PROMPTS DU CORPUS ===\n",
      "\n",
      "'je vais' → je vais vous voir demain\n",
      "'il est' → il est très gentil\n",
      "'elle a' → elle a l'air d'être malade\n",
      "'nous avons' → nous avons des difficultés à l'école\n",
      "'c'est une' → une autre fois\n",
      "'je pense que' → je pense que vous êtes très gentille\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TEST AVEC PROMPTS DU CORPUS ===\\n\")\n",
    "\n",
    "# Prompts plus probables dans Tatoeba\n",
    "test_prompts = [\n",
    "    \"je vais\",\n",
    "    \"il est\",\n",
    "    \"elle a\", \n",
    "    \"nous avons\",\n",
    "    \"c'est une\",\n",
    "    \"je pense que\"\n",
    "]\n",
    "\n",
    "# On s'assure que le modèle est en mode évaluation (pas de dropout)\n",
    "model.eval()\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    \n",
    "    generated = model.generate_text(\n",
    "        prompt, \n",
    "        max_length=10,      \n",
    "        temperature=0.4,    \n",
    "        top_k=10\n",
    "    )\n",
    "    print(f\"'{prompt}' → {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5fe0eb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé sous 'mon_modele_lstm_francais.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Vérifier que le modèle existe bien en mémoire avant de sauvegarder\n",
    "if 'model' in globals():\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        # Sauvegarder tout ce qu'il faut pour reconstruire l'architecture\n",
    "        'vocab_word_to_idx': model.word_to_idx,\n",
    "        'vocab_idx_to_word': model.idx_to_word,\n",
    "        'vocab_size': model.vocab_size,\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'hidden_size': model.hidden_size,\n",
    "        'num_layers': model.num_layers,\n",
    "        'dropout': model.dropout_rate\n",
    "    }\n",
    "    \n",
    "    # C'est CE fichier qu'il faudra charger\n",
    "    torch.save(checkpoint, \"mon_modele_lstm_francais.pth\")\n",
    "    print(\"Modèle sauvegardé sous 'mon_modele_lstm_francais.pth'\")\n",
    "else:\n",
    "    print(\"Erreur : Le modèle n'est pas en mémoire. Tu dois ré-entraîner avant de sauvegarder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a1940801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramètres trouvés :\n",
      "- Embed: 256\n",
      "- Hidden: 512\n",
      "- Vocab: 16434\n",
      "Modèle prêt : vocab=16434, hidden=512, Unidirectionnel\n",
      "\n",
      " Modèle chargé et prêt !\n",
      "Test génération : je suis désolé de vous avoir entraîné là-dedans\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Nom du fichier qu'on vient de créer\n",
    "filename = \"mon_modele_lstm_francais.pth\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    # CHARGEMENT CORRIGÉ\n",
    "    # On précise weights_only=False car on charge aussi la structure du vocabulaire (dictionnaires)\n",
    "    checkpoint = torch.load(filename, map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "    print(\"Paramètres trouvés :\")\n",
    "    print(f\"- Embed: {checkpoint['embedding_dim']}\")\n",
    "    print(f\"- Hidden: {checkpoint['hidden_size']}\")\n",
    "    print(f\"- Vocab: {checkpoint['vocab_size']}\")\n",
    "\n",
    "    # 1. Création de l'instance vide\n",
    "    loaded_model = LSTMLanguageModel(\n",
    "        embedding_dim=checkpoint['embedding_dim'],\n",
    "        hidden_size=checkpoint['hidden_size'],\n",
    "        num_layers=checkpoint['num_layers'],\n",
    "        dropout=checkpoint['dropout']\n",
    "    )\n",
    "\n",
    "    # 2. Restauration du vocabulaire\n",
    "    loaded_model.word_to_idx = checkpoint['vocab_word_to_idx']\n",
    "    loaded_model.idx_to_word = checkpoint['vocab_idx_to_word']\n",
    "    loaded_model.vocab_size = checkpoint['vocab_size']\n",
    "    \n",
    "    # Important : initialiser les couches AVANT de charger les poids\n",
    "    loaded_model._initialize_layers() \n",
    "\n",
    "    # 3. Chargement des poids\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loaded_model.eval()\n",
    "\n",
    "    print(\"\\n Modèle chargé et prêt !\")\n",
    "    print(f\"Test génération : {loaded_model.generate_best('je suis')}\")\n",
    "\n",
    "else:\n",
    "    print(f\" Le fichier '{filename}' est introuvable. As-tu lancé l'étape de sauvegarde ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "64cfd370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " BOT LSTM AMÉLIORÉ (Tape 'quit' pour sortir)\n",
      "   Note: Je complète tes phrases. Essaie de commencer une histoire.\n",
      "============================================================\n",
      "Bot: À la prochaine !\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def chat_with_model_secure(model):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" BOT LSTM AMÉLIORÉ (Tape 'quit' pour sortir)\")\n",
    "    print(\"   Note: Je complète tes phrases. Essaie de commencer une histoire.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nToi (début de phrase) : \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Bot: À la prochaine !\")\n",
    "                break\n",
    "                \n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # --- VÉRIFICATION 1 : Mots inconnus ---\n",
    "            # On vérifie si les mots de l'utilisateur existent dans le vocabulaire\n",
    "            words = user_input.lower().split()\n",
    "            unknown_words = []\n",
    "            for w in words:\n",
    "                # On nettoie la ponctuation basique pour vérifier\n",
    "                clean_w = w.replace(\"'\", \"' \") \n",
    "                if clean_w not in model.word_to_idx and w not in model.word_to_idx:\n",
    "                    # Vérif un peu lâche pour les apostrophes\n",
    "                    if clean_w.split()[0] not in model.word_to_idx:\n",
    "                        unknown_words.append(w)\n",
    "            \n",
    "            if unknown_words:\n",
    "                print(f\"Bot: Désolé, je n'ai jamais appris ces mots : {unknown_words}\")\n",
    "                print(\"     (Mon vocabulaire est limité à ~2000 mots courants)\")\n",
    "                continue\n",
    "\n",
    "            # --- VÉRIFICATION 2 : Longueur ---\n",
    "            if len(words) < 2 and user_input.lower() not in [\"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \"c'est\"]:\n",
    "                print(\"Bot: C'est un peu court... Donne-moi au moins un sujet et un verbe.\")\n",
    "                continue\n",
    "\n",
    "            print(\"Bot (écrit...)\", end=\"\\r\")\n",
    "            \n",
    "            # On utilise generate_best pour éviter le charabia\n",
    "            response = model.generate_best(\n",
    "                user_input, \n",
    "                n_candidates=10, \n",
    "                max_length=15, \n",
    "                temperature=0.6, # Température moyenne pour éviter le délire total\n",
    "                top_k=20\n",
    "            )\n",
    "            \n",
    "            # Affichage propre : on montre la phrase complète\n",
    "            print(f\"Bot complète : \\\"{response}\\\"\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur : {e}\")\n",
    "\n",
    "# Lancer la version sécurisée\n",
    "chat_with_model_secure(loaded_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
