{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36836e33",
   "metadata": {},
   "source": [
    "# TP : LSTM pour Génération de Texte\n",
    "\n",
    "Pipeline : Tokenisation → Vocabulaire → Dataset → Entraînement → Génération → Évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db72e4",
   "metadata": {},
   "source": [
    "## Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2ba5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb7587",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f0680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace('@-@', '-')\n",
    "    text = re.sub(r'=+\\s*.*?\\s*=+', '', text)\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+|\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    punctuations = ['.', ',', ';', ':', '!', '?', ')', '(', '[', ']', '{', '}']\n",
    "    all_words = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        word = word.lower()\n",
    "        while word and word[-1] in punctuations:\n",
    "            if word[:-1]:\n",
    "                all_words.append(word[:-1])\n",
    "            word = word[-1]\n",
    "            if word in punctuations:\n",
    "                all_words.append(word)\n",
    "                word = ''\n",
    "        if word:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    return [w for w in all_words if w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c351a97",
   "metadata": {},
   "source": [
    "## 3. Vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7682f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        self.all_words = sorted([w for w, freq in word_freq.items() if freq >= min_freq])\n",
    "        \n",
    "        # Tokens spéciaux\n",
    "        for idx, token in enumerate(['<pad>', '<unk>', '<bos>', '<eos>']):\n",
    "            self.word2idx[token] = idx\n",
    "            self.idx2word[idx] = token\n",
    "        \n",
    "        for i, word in enumerate(self.all_words, start=4):\n",
    "            self.word2idx[word] = i\n",
    "            self.idx2word[i] = word\n",
    "\n",
    "        self.vocab_size = len(self.all_words) + 4\n",
    "        print(f\"Vocab: {self.vocab_size} mots (min_freq={min_freq})\")\n",
    "\n",
    "    def encode(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>'])\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        return self.idx2word.get(idx, '<unk>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00537a",
   "metadata": {},
   "source": [
    "## Dataset et Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15812129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.pairs = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = ['<bos>'] + tokenize(sentence) + ['<eos>']\n",
    "            encoded_words = [vocab.encode(word) for word in words]\n",
    "            for i in range(len(encoded_words) - seq_len):\n",
    "                self.pairs.append((encoded_words[i:i+seq_len], encoded_words[i+seq_len]))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target = self.pairs[idx]\n",
    "        return torch.tensor(input_seq), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de6e1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, \n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_layer(self.embed(x))\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.linear(self.dropout_layer(output[:, -1, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49fdf8",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eea674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=10, batch_size=32, lr=0.001):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 7\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        perp = math.exp(min(avg_loss, 10))  # Cap pour éviter overflow\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Perplexité: {perp:.2f}\")\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping (epoch {epoch+1})\")\n",
    "            break\n",
    "    \n",
    "    final_perp = math.exp(min(best_loss, 10))\n",
    "    print(f\"\\nMeilleur loss: {best_loss:.4f} - Perplexité: {final_perp:.2f} (epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac886c25",
   "metadata": {},
   "source": [
    "## Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25c65ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(model, vocab, prompt, max_length=100, mode='sampling', temp=0.7, top_k=50, seq_len=20):\n",
    "    model.eval()\n",
    "    prompt_tokens = tokenize(prompt)\n",
    "    tokens = [vocab.encode('<bos>')] + [vocab.encode(word) for word in prompt_tokens]\n",
    "    prompt_length = len(tokens)\n",
    "    unk_id = vocab.encode('<unk>')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            if len(tokens) >= seq_len:\n",
    "                recent = tokens[-seq_len:]\n",
    "            else:\n",
    "                recent = [vocab.encode('<pad>')] * (seq_len - len(tokens)) + tokens\n",
    "                \n",
    "            logits = model(torch.tensor([recent]))\n",
    "            logits[0, unk_id] -= 10.0\n",
    "        \n",
    "            if mode == 'greedy':\n",
    "                next_token = torch.argmax(logits, dim=1).item()\n",
    "            else:  # sampling\n",
    "                if top_k > 0:\n",
    "                    top_vals, top_ids = torch.topk(logits, top_k, dim=1)\n",
    "                    probs = torch.softmax(top_vals / temp, dim=1)\n",
    "                    next_token = top_ids[0, torch.multinomial(probs, 1).item()].item()\n",
    "                else:\n",
    "                    probs = torch.softmax(logits / temp, dim=1)\n",
    "                    next_token = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "            tokens.append(next_token)\n",
    "            if next_token == vocab.encode('<eos>'):\n",
    "                break\n",
    "    \n",
    "    return [vocab.decode(t) for t in tokens[prompt_length:]]\n",
    "\n",
    "def clean_generated_text(words):\n",
    "    return ' '.join([w for w in words if w not in ['<bos>', '<eos>', '<pad>', '<unk>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3f2a",
   "metadata": {},
   "source": [
    "## Chargement et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "664a817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargé: 20741 phrases\n",
      "Configuration optimisée pour perplexité < 80:\n",
      "  Vocab min_freq: 5 (vocabulaire réduit)\n",
      "  Embed: 400, Hidden: 800, Layers: 3\n",
      "  Seq_len: 35, Batch: 128, Epochs: 40\n"
     ]
    }
   ],
   "source": [
    "def load_wiki_tokens(file_path, num_samples=None):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 20:\n",
    "                sentences.append(line)\n",
    "            if num_samples and len(sentences) >= num_samples:\n",
    "                break\n",
    "    print(f\"Chargé: {len(sentences)} phrases\")\n",
    "    return sentences\n",
    "\n",
    "sentences = load_wiki_tokens('wiki.train.tokens', num_samples=None)\n",
    "\n",
    "# Configuration optimisée pour perplexité < 80\n",
    "config = {\n",
    "    'embed_dim': 400,          # Augmenté de 256 à 400\n",
    "    'hidden_dim': 800,         # Augmenté de 512 à 800\n",
    "    'num_layers': 3,\n",
    "    'epochs': 40,              # Augmenté de 25 à 40\n",
    "    'seq_len': 35,             # Augmenté de 20 à 35\n",
    "    'batch_size': 128,         # Augmenté de 64 à 128\n",
    "    'min_word_freq': 5,        # Augmenté de 2 à 5 pour réduire vocab\n",
    "    'max_gen_length': 150,\n",
    "    'temperature': 0.7,\n",
    "    'top_k': 50,\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.5             # Augmenté de 0.3 à 0.5\n",
    "}\n",
    "\n",
    "print(f\"Configuration optimisée pour perplexité < 80:\")\n",
    "print(f\"  Vocab min_freq: {config['min_word_freq']} (vocabulaire réduit)\")\n",
    "print(f\"  Embed: {config['embed_dim']}, Hidden: {config['hidden_dim']}, Layers: {config['num_layers']}\")\n",
    "print(f\"  Seq_len: {config['seq_len']}, Batch: {config['batch_size']}, Epochs: {config['epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ef57e",
   "metadata": {},
   "source": [
    "## Construction et Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f33853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 20397 mots (min_freq=5)\n",
      "Dataset: 1491998 paires\n",
      "Modèle: 38,595,997 paramètres\n",
      "\n",
      "Epoch 1/40 - Loss: 6.4792 - Perplexité: 651.42\n",
      "Epoch 2/40 - Loss: 5.9009 - Perplexité: 365.36\n",
      "Epoch 3/40 - Loss: 5.6555 - Perplexité: 285.85\n",
      "Epoch 4/40 - Loss: 5.4960 - Perplexité: 243.72\n",
      "Epoch 5/40 - Loss: 5.3817 - Perplexité: 217.40\n",
      "Epoch 6/40 - Loss: 5.2910 - Perplexité: 198.54\n",
      "Epoch 7/40 - Loss: 5.2156 - Perplexité: 184.13\n",
      "Epoch 8/40 - Loss: 5.1509 - Perplexité: 172.58\n",
      "Epoch 9/40 - Loss: 5.0951 - Perplexité: 163.22\n",
      "Epoch 10/40 - Loss: 5.0448 - Perplexité: 155.21\n",
      "Epoch 11/40 - Loss: 4.9991 - Perplexité: 148.29\n",
      "Epoch 12/40 - Loss: 4.9582 - Perplexité: 142.34\n",
      "Epoch 13/40 - Loss: 4.9208 - Perplexité: 137.11\n",
      "Epoch 14/40 - Loss: 4.8873 - Perplexité: 132.59\n",
      "Epoch 15/40 - Loss: 4.8579 - Perplexité: 128.75\n",
      "Epoch 16/40 - Loss: 4.8299 - Perplexité: 125.20\n",
      "Epoch 17/40 - Loss: 4.8061 - Perplexité: 122.25\n",
      "Epoch 18/40 - Loss: 4.7865 - Perplexité: 119.88\n",
      "Epoch 19/40 - Loss: 4.7654 - Perplexité: 117.38\n",
      "Epoch 20/40 - Loss: 4.7482 - Perplexité: 115.37\n",
      "Epoch 21/40 - Loss: 4.7305 - Perplexité: 113.35\n",
      "Epoch 22/40 - Loss: 4.7145 - Perplexité: 111.55\n",
      "Epoch 23/40 - Loss: 4.7015 - Perplexité: 110.11\n",
      "Epoch 24/40 - Loss: 4.6866 - Perplexité: 108.49\n",
      "Epoch 25/40 - Loss: 4.6749 - Perplexité: 107.22\n",
      "Epoch 26/40 - Loss: 4.6609 - Perplexité: 105.73\n",
      "Epoch 27/40 - Loss: 4.6504 - Perplexité: 104.63\n",
      "Epoch 28/40 - Loss: 4.6379 - Perplexité: 103.33\n",
      "Epoch 29/40 - Loss: 4.6253 - Perplexité: 102.04\n",
      "Epoch 30/40 - Loss: 4.6189 - Perplexité: 101.38\n",
      "Epoch 31/40 - Loss: 4.6076 - Perplexité: 100.25\n",
      "Epoch 32/40 - Loss: 4.5994 - Perplexité: 99.43\n",
      "Epoch 33/40 - Loss: 4.5913 - Perplexité: 98.62\n",
      "Epoch 34/40 - Loss: 4.5829 - Perplexité: 97.80\n",
      "Epoch 35/40 - Loss: 4.5765 - Perplexité: 97.17\n",
      "Epoch 36/40 - Loss: 4.5702 - Perplexité: 96.56\n",
      "Epoch 37/40 - Loss: 4.5615 - Perplexité: 95.73\n",
      "Epoch 38/40 - Loss: 4.5579 - Perplexité: 95.38\n",
      "Epoch 39/40 - Loss: 4.5499 - Perplexité: 94.63\n",
      "Epoch 40/40 - Loss: 4.5426 - Perplexité: 93.94\n",
      "\n",
      "Meilleur loss: 4.5426 - Perplexité: 93.94 (epoch 40)\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(sentences, min_freq=config['min_word_freq'])\n",
    "\n",
    "dataset = TextDataset(sentences, vocab, seq_len=config['seq_len'])\n",
    "print(f\"Dataset: {len(dataset)} paires\")\n",
    "\n",
    "model = LSTMModel(vocab.vocab_size, config['embed_dim'], config['hidden_dim'], \n",
    "                config['num_layers'], dropout=config['dropout'])\n",
    "print(f\"Modèle: {sum(p.numel() for p in model.parameters()):,} paramètres\\n\")\n",
    "\n",
    "train(model, dataset, epochs=config['epochs'], batch_size=config['batch_size'], lr=config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035a4e0",
   "metadata": {},
   "source": [
    "## Test de Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e98d8f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'The government announced'\n",
      "Génération: that the decision to be a \" more than one \" of the a game . the team 's first international team , which included a number of different players , was at a time in the development of his final appearan...\n",
      "\n",
      "Prompt: 'In recent years'\n",
      "Génération: ....\n",
      "\n",
      "Prompt: 'According to reports'\n",
      "Génération: of the game . the game was originally created on the next two - game games ....\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\"The government announced\", \"In recent years\", \"According to reports\"]:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    \n",
    "    gen = generate_words(model, vocab, prompt, max_length=config['max_gen_length'], \n",
    "                        mode='sampling', temp=config['temperature'], \n",
    "                        top_k=config['top_k'], seq_len=config['seq_len'])\n",
    "    print(f\"Génération: {clean_generated_text(gen)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee16a3",
   "metadata": {},
   "source": [
    "## Évaluation sur Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185ac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[wikinews] 50 prompts\n",
      "  10/50\n",
      "  20/50\n",
      "  30/50\n",
      "  40/50\n",
      "  50/50\n",
      "  Sauvegardé: wikinews_lstm_predictions2.json\n",
      "\n",
      "[wikitext] 50 prompts\n",
      "  10/50\n",
      "  20/50\n",
      "  30/50\n",
      "  40/50\n",
      "  50/50\n",
      "  Sauvegardé: wikitext_lstm_predictions2.json\n",
      "\n",
      "[book] 50 prompts\n",
      "  10/50\n",
      "  20/50\n",
      "  30/50\n",
      "  40/50\n",
      "  50/50\n",
      "  Sauvegardé: book_lstm_predictions2.json\n",
      "\n",
      "Fichiers prêts pour évaluation\n"
     ]
    }
   ],
   "source": [
    "def load_eval_prompts(file_path, num_samples=50):\n",
    "    prompts = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= num_samples:\n",
    "                    break\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    item = data[0] if isinstance(data, list) else data\n",
    "                    prompts.append({'prompt': item.get('prompt', ''), 'gold_ref': item.get('gold_ref', '')})\n",
    "                except:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fichier non trouvé: {file_path}\")\n",
    "    return prompts\n",
    "\n",
    "EVAL_DATASETS = {\n",
    "    'wikinews': 'wikinews_typical-0.95_gpt2-xl_256.jsonl',\n",
    "    'wikitext': 'wikitext_typical-0.95_gpt2-xl_256.jsonl',\n",
    "    'book': 'book_typical-0.95_gpt2-xl_256.jsonl'\n",
    "}\n",
    "\n",
    "for dataset_name, fpath in EVAL_DATASETS.items():\n",
    "    prompts = load_eval_prompts(fpath, num_samples=50)\n",
    "    print(f\"\\n[{dataset_name}] {len(prompts)} prompts\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, item in enumerate(prompts):\n",
    "        generated_results = {}\n",
    "        for pred_idx in range(5):\n",
    "            gen = generate_words(model, vocab, item['prompt'], \n",
    "                               max_length=config['max_gen_length'], \n",
    "                               mode='sampling', temp=config['temperature'],\n",
    "                               top_k=config['top_k'], seq_len=config['seq_len'])\n",
    "            generated_results[str(pred_idx)] = ' '.join(gen)\n",
    "        \n",
    "        results.append({\n",
    "            'prefix_text': item['prompt'],\n",
    "            'reference_text': item['gold_ref'],\n",
    "            'generated_result': generated_results\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  {idx + 1}/{len(prompts)}\")\n",
    "    \n",
    "    output_file = f'{dataset_name}_lstm_predictions2.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"  Sauvegardé: {output_file}\")\n",
    "\n",
    "print(\"\\nFichiers prêts pour évaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0642f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embed): Embedding(20397, 400)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(400, 800, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=800, out_features=20397, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarder uniquement les poids (state_dict)\n",
    "torch.save(model.state_dict(), 'mon_modele_weights.pth')\n",
    "\n",
    "# Charger les poids\n",
    "model.load_state_dict(torch.load('mon_modele_weights.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3601a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé avec succès!\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder votre modèle déjà entraîné\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': vocab.vocab_size,\n",
    "    'embedding_dim': 400,\n",
    "    'hidden_dim': 800,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5\n",
    "}, 'lstm_wikitext_model.pth')\n",
    "\n",
    "print(\"Modèle sauvegardé avec succès!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe576644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé avec succès!\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle\n",
    "checkpoint = torch.load('lstm_wikitext_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Modèle chargé avec succès!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
