# LSTM Text Generation

ü§ñ Impl√©mentation de mod√®les de langage bas√©s sur les r√©seaux LSTM (Long Short-Term Memory) avec PyTorch pour la g√©n√©ration de texte en fran√ßais et en anglais.

## üìã Description

Ce projet explore la g√©n√©ration de texte automatique √† travers l'apprentissage des structures linguistiques par des r√©seaux LSTM. Il propose deux impl√©mentations distinctes :

- **Mod√®le Fran√ßais** : Entra√Æn√© sur 100 000 phrases du corpus Tatoeba
- **Mod√®le Anglais** : Entra√Æn√© sur le dataset WikiText avec optimisation de la perplexit√©

### Fonctionnalit√©s

‚úÖ Architecture LSTM personnalis√©e avec r√©gularisation avanc√©e
‚úÖ Multiples strat√©gies de g√©n√©ration (greedy, sampling, top-k)
‚úÖ Contr√¥le de la temp√©rature et p√©nalit√© de r√©p√©tition
‚úÖ √âvaluation compl√®te avec m√©triques de diversit√© et perplexit√©
‚úÖ Interface interactive (chatbot) pour tester les g√©n√©rations

## üèóÔ∏è Architecture

### Mod√®le Fran√ßais (`LSTMLanguageModel`)
- **Embedding** : 256 dimensions
- **Hidden layers** : 512 unit√©s, 2 couches LSTM
- **Vocabulaire** : ~16 400 mots (fr√©quence min = 2)
- **Dropout** : 0.5 pour r√©gularisation
- **Techniques avanc√©es** :
  - Logic patching pour corrections grammaticales
  - P√©nalit√© de r√©p√©tition adaptative
  - G√©n√©ration best-of-N avec scoring heuristique

### Mod√®le Anglais (`LSTMModel`)
- **Embedding** : 400 dimensions
- **Hidden layers** : 800 unit√©s, 3 couches LSTM
- **Vocabulaire** : ~20 400 mots (fr√©quence min = 5)
- **Dropout** : 0.5
- **Perplexit√© atteinte** : 93.94

## üìä R√©sultats

### Mod√®le Fran√ßais
```
Exemples de g√©n√©ration :
- "je suis" ‚Üí "je suis d√©sol√© de ne pas te voir"
- "je vais" ‚Üí "je vais vous voir demain"
- "il est" ‚Üí "il est tr√®s gentil"
```

### Mod√®le Anglais - M√©triques d'√©valuation

| Dataset   | Diversit√© Ref | Diversit√© Pred | Longueur Moy |
|-----------|---------------|----------------|--------------|
| WikiNews  | 92.1%         | 90.5%          | 53.2 tokens  |
| WikiText  | 91.8%         | 90.6%          | 52.8 tokens  |
| Book      | 93.2%         | 90.5%          | 53.1 tokens  |

**Points forts** :
- Excellente diversit√© (~90.5%) proche des textes de r√©f√©rence
- G√©n√©ration stable avec longueur coh√©rente
- Robustesse entre diff√©rents domaines

## üöÄ Installation

### Pr√©requis
```bash
Python 3.8+
PyTorch 2.0+
```

### D√©pendances
```bash
pip install torch numpy matplotlib pandas
```

## üìñ Utilisation

### 1. Mod√®le Fran√ßais

Ouvrez `LSTMLanguageModel.ipynb` et ex√©cutez les cellules dans l'ordre :

```python
# Charger le mod√®le pr√©-entra√Æn√©
checkpoint = torch.load("mon_modele_lstm_francais.pth",
                        map_location=torch.device('cpu'),
                        weights_only=False)

# G√©n√©rer du texte
model.generate_text("je suis", max_length=15, temperature=0.7)
```

**Options de g√©n√©ration** :
- `max_length` : Nombre maximum de mots √† g√©n√©rer
- `temperature` : Contr√¥le de la cr√©ativit√© (0.5-1.0)
- `top_k` : Restriction aux k mots les plus probables
- `repetition_penalty` : P√©nalit√© pour √©viter les r√©p√©titions (1.0-2.0)

### 2. Mod√®le Anglais

Ouvrez `TextGen_LSTM.ipynb` :

```python
# Charger le mod√®le
checkpoint = torch.load('lstm_wikitext_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# G√©n√©rer avec sampling
generate_words(model, vocab, "The government announced",
               max_length=100, mode='sampling', temp=0.7, top_k=50)
```

### 3. √âvaluation des r√©sultats

Consultez `LSTM_Evaluation_Results.ipynb` pour voir :
- Graphiques de comparaison de diversit√©
- Analyses statistiques de longueur de g√©n√©ration
- M√©triques d√©taill√©es par dataset

## üìÅ Structure du Projet

```
lstm-text-generation/
‚îú‚îÄ‚îÄ LSTMLanguageModel.ipynb         # Mod√®le fran√ßais avec interface interactive
‚îú‚îÄ‚îÄ TextGen_LSTM.ipynb              # Mod√®le anglais + √©valuation
‚îú‚îÄ‚îÄ LSTM_Evaluation_Results.ipynb   # Visualisation des r√©sultats
‚îú‚îÄ‚îÄ mon_modele_lstm_francais.pth    # Checkpoint mod√®le fran√ßais
‚îú‚îÄ‚îÄ lstm_wikitext_model.pth         # Checkpoint mod√®le anglais
‚îî‚îÄ‚îÄ README.md
```

## üî¨ M√©thodologie

### Pipeline d'entra√Ænement
1. **Nettoyage du corpus** : Filtrage, tokenisation, normalisation
2. **Construction du vocabulaire** : Tokens sp√©ciaux (<BOS>, <EOS>, <PAD>, <UNK>)
3. **Pr√©paration des donn√©es** : Cr√©ation de paires (input, target) avec padding
4. **Entra√Ænement** :
   - Optimiseur AdamW avec weight decay
   - Learning rate scheduler (ReduceLROnPlateau)
   - Gradient clipping (max_norm=1.0)
   - Early stopping (patience=7)
5. **G√©n√©ration** : Sampling avec temp√©rature et top-k filtering

### Techniques de r√©gularisation
- Dropout (0.5) entre les couches LSTM
- Weight decay (0.001)
- Gradient clipping
- Vocabulaire filtr√© par fr√©quence

## üéØ Exemples d'utilisation avanc√©e

### G√©n√©ration best-of-N (mod√®le fran√ßais)
```python
# G√©n√®re 10 candidats et s√©lectionne le meilleur selon des crit√®res heuristiques
model.generate_best("je suis", n_candidates=10, max_length=15,
                    temperature=0.7, top_k=15)
```

### Interface de chat interactif
```python
# Chatbot interactif avec v√©rification du vocabulaire
chat_with_model_secure(loaded_model)
```

## üìà Am√©liorations Futures

- [ ] Augmenter la taille du corpus d'entra√Ænement
- [ ] Impl√©menter l'attention mechanism
- [ ] Tester des architectures Transformer
- [ ] Ajouter le beam search pour la g√©n√©ration
- [ ] Fine-tuning sur des domaines sp√©cifiques
- [ ] √âvaluation BLEU et ROUGE pour la qualit√©

## üìù Datasets Utilis√©s

- **Tatoeba** : Phrases fran√ßaises (100 000 phrases)
- **WikiText** : Articles Wikip√©dia anglais (20 741 phrases)
- **√âvaluation** : WikiNews, WikiText, BookCorpus

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.

## üìÑ Licence

Ce projet est sous licence MIT.

## üë§ Auteur

Guillaume - [Guillaumevr79](https://github.com/Guillaumevr79)

---

‚≠ê Si ce projet vous est utile, n'h√©sitez pas √† lui donner une √©toile !
